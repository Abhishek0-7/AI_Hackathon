{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJxZHBs1375cxf/I64kmD6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load the datasets with error handling for bad rows\n",
        "operations_labels_train = pd.read_csv('/content/operations_labels_training.csv')\n",
        "\n",
        "# Use on_bad_lines='skip' to skip problematic rows\n",
        "telemetry_train = pd.read_csv('/content/telemetry_for_operations_training.csv', on_bad_lines='skip')\n",
        "\n",
        "# Validation data is unlikely to have issues, but we can still add on_bad_lines just in case\n",
        "telemetry_validation = pd.read_csv('/content/telemetry_for_operations_validation.csv', on_bad_lines='skip')\n",
        "\n",
        "# Convert time columns to datetime\n",
        "operations_labels_train['start_time'] = pd.to_datetime(operations_labels_train['start_time'], errors='coerce')\n",
        "operations_labels_train['end_time'] = pd.to_datetime(operations_labels_train['end_time'], errors='coerce')\n",
        "telemetry_train['create_dt'] = pd.to_datetime(telemetry_train['create_dt'], errors='coerce')\n",
        "telemetry_validation['create_dt'] = pd.to_datetime(telemetry_validation['create_dt'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid datetimes in the training set\n",
        "telemetry_train = telemetry_train.dropna(subset=['create_dt'])\n",
        "\n",
        "# Merge telemetry data with labels based on time intervals using a more efficient approach\n",
        "telemetry_train = telemetry_train.merge(\n",
        "    operations_labels_train,\n",
        "    how='left',\n",
        "    left_on=['mdm_object_name', 'create_dt'],\n",
        "    right_on=['mdm_object_name', 'start_time'],\n",
        "    suffixes=('', '_label')\n",
        ")\n",
        "\n",
        "# Now filter based on the time conditions\n",
        "telemetry_train['operation_kind_id'] = telemetry_train.apply(\n",
        "    lambda row: row['operation_kind_id'] if row['start_time'] <= row['create_dt'] <= row['end_time'] else None,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Drop rows where no operation_kind_id could be assigned\n",
        "telemetry_train = telemetry_train.dropna(subset=['operation_kind_id'])\n",
        "\n",
        "# Convert operation_kind_id to integers\n",
        "telemetry_train.loc[:, 'operation_kind_id'] = telemetry_train['operation_kind_id'].astype(int)\n",
        "\n",
        "# Feature Engineering - Extract time-based features (hour, day of week)\n",
        "telemetry_train['hour'] = telemetry_train['create_dt'].dt.hour\n",
        "telemetry_train['dayofweek'] = telemetry_train['create_dt'].dt.dayofweek\n",
        "telemetry_validation['hour'] = telemetry_validation['create_dt'].dt.hour\n",
        "telemetry_validation['dayofweek'] = telemetry_validation['create_dt'].dt.dayofweek\n",
        "\n",
        "# Drop datetime and non-numeric columns before imputation and scaling\n",
        "telemetry_train_cleaned = telemetry_train.drop(columns=['create_dt', 'mdm_object_name', 'start_time', 'end_time'])\n",
        "\n",
        "# Ensure only numeric columns are retained\n",
        "telemetry_train_cleaned = telemetry_train_cleaned.select_dtypes(include=[np.number])\n",
        "\n",
        "# Split the features and target variable\n",
        "X_train_features = telemetry_train_cleaned.drop(columns=['operation_kind_id'])\n",
        "y_train_target = telemetry_train_cleaned['operation_kind_id']\n",
        "\n",
        "# Handling missing values for numeric columns only\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_features), columns=X_train_features.columns)\n",
        "\n",
        "# Standardization of features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=X_train_imputed.columns)\n",
        "\n",
        "# Feature Selection using RandomForest\n",
        "rf_feature_selector = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "rf_feature_selector.fit(X_train_scaled, y_train_target)\n",
        "\n",
        "# Select features based on importance\n",
        "model = SelectFromModel(rf_feature_selector, prefit=True)\n",
        "X_selected = model.transform(X_train_scaled)\n",
        "\n",
        "# Prepare data for model training\n",
        "X = pd.DataFrame(X_selected)\n",
        "y = y_train_target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "### Hyperparameter Tuning with RandomizedSearchCV for RandomForest\n",
        "param_dist_rf = {\n",
        "    'n_estimators': randint(50, 100),\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': randint(2, 5),\n",
        "    'min_samples_leaf': randint(1, 3)\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "random_search_rf = RandomizedSearchCV(estimator=rf, param_distributions=param_dist_rf, n_iter=5, cv=3, n_jobs=-1, scoring='f1_weighted', random_state=42)\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "best_rf = random_search_rf.best_estimator_\n",
        "\n",
        "# Model Ensemble - RandomForest and GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier(random_state=42)\n",
        "voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('gbc', gbc)], voting='soft', n_jobs=-1)\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Cross-validation using 3-fold KFold\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "cv_scores = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "    voting_clf.fit(X_train_fold, y_train_fold)\n",
        "    y_pred_fold = voting_clf.predict(X_test_fold)\n",
        "    cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))\n",
        "\n",
        "print(f'Mean Cross-Validation F1 Score: {np.mean(cv_scores)}')\n",
        "\n",
        "# Predict on the test set and evaluate the model\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f'Weighted F1 Score on Test Set: {f1}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Clean and preprocess the validation set\n",
        "telemetry_validation_cleaned = telemetry_validation.drop(columns=['create_dt', 'mdm_object_name'])\n",
        "\n",
        "# Ensure only numeric columns are retained and match the training set\n",
        "telemetry_validation_cleaned = telemetry_validation_cleaned.select_dtypes(include=[np.number])\n",
        "\n",
        "# Match the validation set columns to the training set\n",
        "columns_to_keep = X_train_features.columns\n",
        "telemetry_validation_cleaned = telemetry_validation_cleaned.reindex(columns=columns_to_keep, fill_value=0)\n",
        "\n",
        "# Impute and scale the validation set\n",
        "telemetry_validation_imputed = pd.DataFrame(imputer.transform(telemetry_validation_cleaned), columns=telemetry_validation_cleaned.columns)\n",
        "telemetry_validation_scaled = pd.DataFrame(scaler.transform(telemetry_validation_imputed), columns=telemetry_validation_imputed.columns)\n",
        "\n",
        "# Select features for the validation set\n",
        "X_validation_selected = model.transform(telemetry_validation_scaled)\n",
        "\n",
        "# Predicting on validation set\n",
        "validation_preds = voting_clf.predict(X_validation_selected)\n",
        "\n",
        "# Ensure the original validation set length (260111 rows)\n",
        "submission = pd.DataFrame({\n",
        "    'create_dt': telemetry_validation['create_dt'],\n",
        "    'mdm_object_name': telemetry_validation['mdm_object_name'],\n",
        "    'operation_kind_id': validation_preds\n",
        "})\n",
        "\n",
        "# Ensure the submission file has 260111 rows\n",
        "assert submission.shape[0] == 260111, \"Submission file does not contain the correct number of rows.\"\n",
        "\n",
        "# Save the submission with Windows-friendly line terminators\n",
        "submission.to_csv('submission.csv', index=False, lineterminator='\\r\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HPdYeqL9G7Z",
        "outputId": "aea2fe22-d6b5-4366-e939-4bb9fb380781"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-ec3c313869cb>:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  telemetry_train['hour'] = telemetry_train['create_dt'].dt.hour\n",
            "<ipython-input-6-ec3c313869cb>:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  telemetry_train['dayofweek'] = telemetry_train['create_dt'].dt.dayofweek\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Cross-Validation F1 Score: 0.3836508836594053\n",
            "Weighted F1 Score on Test Set: 0.6704197349826397\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.54      0.61       136\n",
            "         1.0       0.62      0.64      0.63       171\n",
            "         2.0       0.61      0.63      0.62       167\n",
            "         3.0       0.66      0.87      0.75       139\n",
            "         5.0       0.78      0.68      0.73       226\n",
            "\n",
            "    accuracy                           0.67       839\n",
            "   macro avg       0.67      0.67      0.67       839\n",
            "weighted avg       0.68      0.67      0.67       839\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}